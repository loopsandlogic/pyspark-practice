{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f64fc022",
   "metadata": {},
   "source": [
    "# Employee Data\n",
    "\n",
    "- employee_id\n",
    "- first_name\n",
    "- last_name\n",
    "- phone_num\n",
    "    - country_code\n",
    "    - area_code\n",
    "    - exchange_code\n",
    "    - subscriber_number\n",
    "- email_id\n",
    "- address\n",
    "    - street_name\n",
    "    - unit_number\n",
    "    - city\n",
    "    - county\n",
    "    - state\n",
    "    - zip_code\n",
    "    - extended_zip_code\n",
    "        - sector\n",
    "        - segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "8a326e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required modules/libraries\n",
    "from pyspark.sql.types import (StructType, StructField, StringType, IntegerType)\n",
    "from pyspark.sql import SparkSession, Row\n",
    "from datetime import date, datetime\n",
    "from pyspark.sql.functions import col, struct\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "830d8e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark connection\n",
    "spark = SparkSession.builder.appName(\"Employee Analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "915665c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Employee Schema\n",
    "\n",
    "extended_zip_code_schema = StructType([\n",
    "    StructField(\"sector\", IntegerType()),  # Example: 54\n",
    "    StructField(\"segment\", IntegerType()) # Example: 01\n",
    "])\n",
    "\n",
    "address_schema = StructType([\n",
    "    StructField(\"street_name\", StringType(), False),\n",
    "    StructField(\"unit_number\", StringType()),\n",
    "    StructField(\"city\", StringType(), False),\n",
    "    StructField(\"county\", StringType()),\n",
    "    StructField(\"state\", StringType(), False),\n",
    "    StructField(\"zip_code\", IntegerType(), False),\n",
    "    StructField(\"extended_zip_code\", extended_zip_code_schema) # Nesting the previous structure\n",
    "])\n",
    "\n",
    "phone_num_schema = StructType([                                 # e.g., 1 (555)-123-4567\n",
    "    StructField(\"country_code\", IntegerType(), False),          # e.g., \"1\"\n",
    "    StructField(\"area_code\", IntegerType(), False),             # e.g., \"555\"\n",
    "    StructField(\"exchange_code\", IntegerType(), False),         # e.g., \"123\"\n",
    "    StructField(\"subscriber_number\", IntegerType(), False)      # e.g., \"4567\"\n",
    "])\n",
    "\n",
    "employee_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), False),\n",
    "    StructField(\"dept_id\", IntegerType(), False),    \n",
    "    StructField(\"first_name\", StringType(), False),\n",
    "    StructField(\"middle_name\", StringType()),\n",
    "    StructField(\"last_name\", StringType(), False),\n",
    "    StructField(\"phone_num\", phone_num_schema, False),         # Nested Phone Structure\n",
    "    StructField(\"email_id\", StringType(), False),\n",
    "    StructField(\"address\", address_schema, False)              # Nested Address Structure\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "8df6d8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Department Schema\n",
    "\n",
    "dept_schema = StructType([\n",
    "    StructField(\"dept_id\", IntegerType(), False),\n",
    "    StructField(\"dept_name\", StringType(), False),\n",
    "    StructField(\"dept_head_emp_id\", IntegerType(), False),\n",
    "    StructField(\"loc_address\", address_schema, False)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "70df117a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data\n",
    "\n",
    "FIRST_NAMES = [\"Alice\", \"Bob\", \"Charlie\", \"Diana\", \"Ethan\", \"Fiona\", \"George\", \"Hannah\", \"Ivy\", \"Jack\"]\n",
    "LAST_NAMES = [\"Smith\", \"Jones\", \"Williams\", \"Brown\", \"Davis\", \"Miller\", \"Wilson\", \"Moore\", \"Taylor\", \"Anderson\"]\n",
    "CITIES = [\"New York\", \"Chicago\", \"Boston\", \"Seattle\", \"Austin\", \"Denver\"]\n",
    "STATES = [\"NY\", \"IL\", \"MA\", \"WA\", \"TX\", \"CO\"]\n",
    "DEPARTMENTS = [\n",
    "    (10, \"Engineering\"),\n",
    "    (20, \"Sales\"),\n",
    "    (30, \"HR\"),\n",
    "    (40, \"Finance\"),\n",
    "    (50, \"Marketing\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "1c786b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of department IDs that we will assign to employees\n",
    "DEPT_IDS = [id for id, name in DEPARTMENTS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "f8048dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_address(city, state):\n",
    "    \"\"\"Generates a nested address Row consistent with address_schema.\"\"\"\n",
    "    zip_code_base = random.randint(10000, 99999)\n",
    "    return Row(\n",
    "        street_name=f\"{random.randint(100, 999)} {random.choice(['Oak', 'Pine', 'Main'])} St\",\n",
    "        unit_number=random.choice([None, str(random.randint(1, 200))]),\n",
    "        city=city,\n",
    "        county=f\"{city} County\",\n",
    "        state=state,\n",
    "        zip_code=zip_code_base,\n",
    "        extended_zip_code=Row(\n",
    "            sector=random.randint(10, 99),\n",
    "            segment=random.randint(10, 99)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f4bb7e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dept_data = []\n",
    "# Ensure each department gets a location\n",
    "for dept_id, dept_name in DEPARTMENTS:\n",
    "    city = random.choice(CITIES)\n",
    "    state = STATES[CITIES.index(city)]\n",
    "    \n",
    "    # Generate a unique employee ID for the department head (ensuring it's not the same as a future employee ID)\n",
    "    dept_head_emp_id = 1000 + dept_id\n",
    "    \n",
    "    dept_data.append(Row(\n",
    "        dept_id=dept_id,\n",
    "        dept_name=dept_name,\n",
    "        dept_head_emp_id=dept_head_emp_id,\n",
    "        loc_address=generate_address(city, state)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "86aa8c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(dept_id=10, dept_name='Engineering', dept_head_emp_id=9010, loc_address=Row(street_name='666 Pine St', unit_number='197', city='New York', county='New York County', state='NY', zip_code=20930, extended_zip_code=Row(sector=72, segment=12))), Row(dept_id=20, dept_name='Sales', dept_head_emp_id=9020, loc_address=Row(street_name='403 Main St', unit_number='42', city='Boston', county='Boston County', state='MA', zip_code=58471, extended_zip_code=Row(sector=93, segment=30))), Row(dept_id=30, dept_name='HR', dept_head_emp_id=9030, loc_address=Row(street_name='592 Main St', unit_number='165', city='New York', county='New York County', state='NY', zip_code=49000, extended_zip_code=Row(sector=83, segment=10))), Row(dept_id=40, dept_name='Finance', dept_head_emp_id=9040, loc_address=Row(street_name='644 Main St', unit_number=None, city='Chicago', county='Chicago County', state='IL', zip_code=13146, extended_zip_code=Row(sector=36, segment=33))), Row(dept_id=50, dept_name='Marketing', dept_head_emp_id=9050, loc_address=Row(street_name='333 Main St', unit_number='93', city='Chicago', county='Chicago County', state='IL', zip_code=56450, extended_zip_code=Row(sector=47, segment=72)))]\n"
     ]
    }
   ],
   "source": [
    "print(dept_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "61bc5e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(employee_id=1001, dept_id=30, first_name='Hannah', middle_name=None, last_name='Taylor', phone_num=Row(country_code=1, area_code=600, exchange_code=372, subscriber_number=2883), email_id='hannah.taylor@company.com', address=Row(street_name='530 Pine St', unit_number=None, city='Boston', county='Boston County', state='MA', zip_code=30932, extended_zip_code=Row(sector=94, segment=54))), Row(employee_id=1002, dept_id=10, first_name='Alice', middle_name='Xavier', last_name='Moore', phone_num=Row(country_code=1, area_code=979, exchange_code=942, subscriber_number=8967), email_id='alice.moore@company.com', address=Row(street_name='126 Oak St', unit_number=None, city='New York', county='New York County', state='NY', zip_code=37130, extended_zip_code=Row(sector=20, segment=13))), Row(employee_id=1003, dept_id=50, first_name='Charlie', middle_name=None, last_name='Brown', phone_num=Row(country_code=1, area_code=640, exchange_code=291, subscriber_number=4687), email_id='charlie.brown@company.com', address=Row(street_name='299 Oak St', unit_number=None, city='Austin', county='Austin County', state='TX', zip_code=11811, extended_zip_code=Row(sector=48, segment=17))), Row(employee_id=1004, dept_id=30, first_name='Ethan', middle_name='Xavier', last_name='Davis', phone_num=Row(country_code=1, area_code=899, exchange_code=213, subscriber_number=4400), email_id='ethan.davis@company.com', address=Row(street_name='744 Oak St', unit_number='105', city='Boston', county='Boston County', state='MA', zip_code=42324, extended_zip_code=Row(sector=70, segment=51))), Row(employee_id=1005, dept_id=20, first_name='Bob', middle_name=None, last_name='Moore', phone_num=Row(country_code=1, area_code=817, exchange_code=117, subscriber_number=4435), email_id='bob.moore@company.com', address=Row(street_name='760 Pine St', unit_number='116', city='Chicago', county='Chicago County', state='IL', zip_code=72108, extended_zip_code=Row(sector=14, segment=70))), Row(employee_id=1006, dept_id=40, first_name='Bob', middle_name='Yancy', last_name='Moore', phone_num=Row(country_code=1, area_code=474, exchange_code=519, subscriber_number=6813), email_id='bob.moore@company.com', address=Row(street_name='999 Oak St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=98712, extended_zip_code=Row(sector=87, segment=56))), Row(employee_id=1007, dept_id=10, first_name='Hannah', middle_name=None, last_name='Taylor', phone_num=Row(country_code=1, area_code=910, exchange_code=684, subscriber_number=8785), email_id='hannah.taylor@company.com', address=Row(street_name='718 Pine St', unit_number='179', city='Austin', county='Austin County', state='TX', zip_code=89820, extended_zip_code=Row(sector=47, segment=35))), Row(employee_id=1008, dept_id=30, first_name='Alice', middle_name='Yancy', last_name='Davis', phone_num=Row(country_code=1, area_code=312, exchange_code=782, subscriber_number=7781), email_id='alice.davis@company.com', address=Row(street_name='155 Oak St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=75096, extended_zip_code=Row(sector=83, segment=93))), Row(employee_id=1009, dept_id=30, first_name='Jack', middle_name='Xavier', last_name='Williams', phone_num=Row(country_code=1, area_code=630, exchange_code=321, subscriber_number=5364), email_id='jack.williams@company.com', address=Row(street_name='207 Main St', unit_number='46', city='Chicago', county='Chicago County', state='IL', zip_code=90878, extended_zip_code=Row(sector=89, segment=52))), Row(employee_id=1010, dept_id=40, first_name='Jack', middle_name='Xavier', last_name='Taylor', phone_num=Row(country_code=1, area_code=971, exchange_code=878, subscriber_number=3574), email_id='jack.taylor@company.com', address=Row(street_name='986 Main St', unit_number='93', city='Austin', county='Austin County', state='TX', zip_code=44724, extended_zip_code=Row(sector=62, segment=23))), Row(employee_id=1011, dept_id=50, first_name='Alice', middle_name=None, last_name='Moore', phone_num=Row(country_code=1, area_code=296, exchange_code=278, subscriber_number=7752), email_id='alice.moore@company.com', address=Row(street_name='864 Oak St', unit_number='38', city='Denver', county='Denver County', state='CO', zip_code=84281, extended_zip_code=Row(sector=85, segment=59))), Row(employee_id=1012, dept_id=50, first_name='Alice', middle_name=None, last_name='Miller', phone_num=Row(country_code=1, area_code=513, exchange_code=441, subscriber_number=7931), email_id='alice.miller@company.com', address=Row(street_name='943 Oak St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=62834, extended_zip_code=Row(sector=77, segment=70))), Row(employee_id=1013, dept_id=20, first_name='Bob', middle_name='Yancy', last_name='Davis', phone_num=Row(country_code=1, area_code=940, exchange_code=849, subscriber_number=4932), email_id='bob.davis@company.com', address=Row(street_name='624 Oak St', unit_number=None, city='Seattle', county='Seattle County', state='WA', zip_code=21874, extended_zip_code=Row(sector=73, segment=83))), Row(employee_id=1014, dept_id=50, first_name='Ethan', middle_name=None, last_name='Wilson', phone_num=Row(country_code=1, area_code=448, exchange_code=972, subscriber_number=4364), email_id='ethan.wilson@company.com', address=Row(street_name='908 Oak St', unit_number='54', city='New York', county='New York County', state='NY', zip_code=34579, extended_zip_code=Row(sector=98, segment=89))), Row(employee_id=1015, dept_id=20, first_name='Alice', middle_name=None, last_name='Anderson', phone_num=Row(country_code=1, area_code=870, exchange_code=500, subscriber_number=2319), email_id='alice.anderson@company.com', address=Row(street_name='617 Pine St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=96897, extended_zip_code=Row(sector=29, segment=61))), Row(employee_id=1016, dept_id=50, first_name='Alice', middle_name=None, last_name='Anderson', phone_num=Row(country_code=1, area_code=500, exchange_code=524, subscriber_number=1910), email_id='alice.anderson@company.com', address=Row(street_name='378 Oak St', unit_number='33', city='Denver', county='Denver County', state='CO', zip_code=93805, extended_zip_code=Row(sector=71, segment=78))), Row(employee_id=1017, dept_id=10, first_name='Fiona', middle_name=None, last_name='Miller', phone_num=Row(country_code=1, area_code=804, exchange_code=111, subscriber_number=9261), email_id='fiona.miller@company.com', address=Row(street_name='153 Oak St', unit_number='46', city='Chicago', county='Chicago County', state='IL', zip_code=93964, extended_zip_code=Row(sector=38, segment=60))), Row(employee_id=1018, dept_id=40, first_name='George', middle_name='Yancy', last_name='Smith', phone_num=Row(country_code=1, area_code=474, exchange_code=840, subscriber_number=7410), email_id='george.smith@company.com', address=Row(street_name='897 Oak St', unit_number='119', city='Chicago', county='Chicago County', state='IL', zip_code=54978, extended_zip_code=Row(sector=42, segment=81))), Row(employee_id=1019, dept_id=30, first_name='Bob', middle_name='Xavier', last_name='Smith', phone_num=Row(country_code=1, area_code=808, exchange_code=338, subscriber_number=9707), email_id='bob.smith@company.com', address=Row(street_name='708 Pine St', unit_number=None, city='New York', county='New York County', state='NY', zip_code=89831, extended_zip_code=Row(sector=72, segment=33))), Row(employee_id=1020, dept_id=50, first_name='Alice', middle_name='Xavier', last_name='Anderson', phone_num=Row(country_code=1, area_code=554, exchange_code=681, subscriber_number=4325), email_id='alice.anderson@company.com', address=Row(street_name='731 Oak St', unit_number=None, city='Chicago', county='Chicago County', state='IL', zip_code=99270, extended_zip_code=Row(sector=76, segment=71))), Row(employee_id=1021, dept_id=30, first_name='Charlie', middle_name=None, last_name='Davis', phone_num=Row(country_code=1, area_code=438, exchange_code=864, subscriber_number=6129), email_id='charlie.davis@company.com', address=Row(street_name='621 Main St', unit_number=None, city='Boston', county='Boston County', state='MA', zip_code=16424, extended_zip_code=Row(sector=78, segment=94))), Row(employee_id=1022, dept_id=20, first_name='Fiona', middle_name='Xavier', last_name='Davis', phone_num=Row(country_code=1, area_code=881, exchange_code=558, subscriber_number=3701), email_id='fiona.davis@company.com', address=Row(street_name='719 Pine St', unit_number=None, city='Austin', county='Austin County', state='TX', zip_code=47870, extended_zip_code=Row(sector=99, segment=90))), Row(employee_id=1023, dept_id=30, first_name='Jack', middle_name=None, last_name='Smith', phone_num=Row(country_code=1, area_code=682, exchange_code=604, subscriber_number=5496), email_id='jack.smith@company.com', address=Row(street_name='367 Oak St', unit_number='140', city='Austin', county='Austin County', state='TX', zip_code=56407, extended_zip_code=Row(sector=14, segment=11))), Row(employee_id=1024, dept_id=20, first_name='Charlie', middle_name=None, last_name='Davis', phone_num=Row(country_code=1, area_code=789, exchange_code=135, subscriber_number=5429), email_id='charlie.davis@company.com', address=Row(street_name='590 Oak St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=58339, extended_zip_code=Row(sector=58, segment=52))), Row(employee_id=1025, dept_id=20, first_name='George', middle_name='Xavier', last_name='Wilson', phone_num=Row(country_code=1, area_code=791, exchange_code=211, subscriber_number=8188), email_id='george.wilson@company.com', address=Row(street_name='396 Pine St', unit_number='9', city='Seattle', county='Seattle County', state='WA', zip_code=39298, extended_zip_code=Row(sector=59, segment=90))), Row(employee_id=1026, dept_id=20, first_name='Charlie', middle_name='Yancy', last_name='Williams', phone_num=Row(country_code=1, area_code=655, exchange_code=239, subscriber_number=4695), email_id='charlie.williams@company.com', address=Row(street_name='183 Main St', unit_number='131', city='Denver', county='Denver County', state='CO', zip_code=19957, extended_zip_code=Row(sector=46, segment=18))), Row(employee_id=1027, dept_id=50, first_name='George', middle_name='Yancy', last_name='Wilson', phone_num=Row(country_code=1, area_code=681, exchange_code=133, subscriber_number=7849), email_id='george.wilson@company.com', address=Row(street_name='407 Oak St', unit_number=None, city='Chicago', county='Chicago County', state='IL', zip_code=45113, extended_zip_code=Row(sector=30, segment=45))), Row(employee_id=1028, dept_id=40, first_name='Diana', middle_name='Yancy', last_name='Wilson', phone_num=Row(country_code=1, area_code=257, exchange_code=966, subscriber_number=2071), email_id='diana.wilson@company.com', address=Row(street_name='776 Pine St', unit_number='109', city='Boston', county='Boston County', state='MA', zip_code=84015, extended_zip_code=Row(sector=67, segment=36))), Row(employee_id=1029, dept_id=30, first_name='Diana', middle_name=None, last_name='Davis', phone_num=Row(country_code=1, area_code=688, exchange_code=940, subscriber_number=5422), email_id='diana.davis@company.com', address=Row(street_name='842 Pine St', unit_number='12', city='New York', county='New York County', state='NY', zip_code=79382, extended_zip_code=Row(sector=39, segment=65))), Row(employee_id=1030, dept_id=40, first_name='Diana', middle_name=None, last_name='Williams', phone_num=Row(country_code=1, area_code=498, exchange_code=961, subscriber_number=5198), email_id='diana.williams@company.com', address=Row(street_name='904 Main St', unit_number=None, city='Austin', county='Austin County', state='TX', zip_code=89145, extended_zip_code=Row(sector=73, segment=23))), Row(employee_id=1031, dept_id=10, first_name='Hannah', middle_name=None, last_name='Miller', phone_num=Row(country_code=1, area_code=525, exchange_code=932, subscriber_number=8923), email_id='hannah.miller@company.com', address=Row(street_name='741 Main St', unit_number='66', city='Seattle', county='Seattle County', state='WA', zip_code=38452, extended_zip_code=Row(sector=26, segment=57))), Row(employee_id=1032, dept_id=40, first_name='Bob', middle_name='Xavier', last_name='Davis', phone_num=Row(country_code=1, area_code=883, exchange_code=960, subscriber_number=6690), email_id='bob.davis@company.com', address=Row(street_name='319 Oak St', unit_number=None, city='Chicago', county='Chicago County', state='IL', zip_code=64183, extended_zip_code=Row(sector=46, segment=89))), Row(employee_id=1033, dept_id=10, first_name='George', middle_name=None, last_name='Anderson', phone_num=Row(country_code=1, area_code=338, exchange_code=371, subscriber_number=9120), email_id='george.anderson@company.com', address=Row(street_name='597 Oak St', unit_number=None, city='Boston', county='Boston County', state='MA', zip_code=72651, extended_zip_code=Row(sector=57, segment=69))), Row(employee_id=1034, dept_id=40, first_name='George', middle_name='Yancy', last_name='Miller', phone_num=Row(country_code=1, area_code=240, exchange_code=502, subscriber_number=5326), email_id='george.miller@company.com', address=Row(street_name='519 Main St', unit_number=None, city='Chicago', county='Chicago County', state='IL', zip_code=34968, extended_zip_code=Row(sector=17, segment=73))), Row(employee_id=1035, dept_id=40, first_name='Fiona', middle_name='Yancy', last_name='Brown', phone_num=Row(country_code=1, area_code=703, exchange_code=277, subscriber_number=7167), email_id='fiona.brown@company.com', address=Row(street_name='275 Pine St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=72632, extended_zip_code=Row(sector=20, segment=23))), Row(employee_id=1036, dept_id=10, first_name='Hannah', middle_name='Yancy', last_name='Taylor', phone_num=Row(country_code=1, area_code=252, exchange_code=210, subscriber_number=2460), email_id='hannah.taylor@company.com', address=Row(street_name='761 Pine St', unit_number=None, city='Seattle', county='Seattle County', state='WA', zip_code=25823, extended_zip_code=Row(sector=61, segment=41))), Row(employee_id=1037, dept_id=10, first_name='Jack', middle_name='Yancy', last_name='Moore', phone_num=Row(country_code=1, area_code=937, exchange_code=305, subscriber_number=3406), email_id='jack.moore@company.com', address=Row(street_name='804 Main St', unit_number='24', city='Austin', county='Austin County', state='TX', zip_code=47418, extended_zip_code=Row(sector=36, segment=98))), Row(employee_id=1038, dept_id=10, first_name='Charlie', middle_name='Yancy', last_name='Taylor', phone_num=Row(country_code=1, area_code=546, exchange_code=667, subscriber_number=6695), email_id='charlie.taylor@company.com', address=Row(street_name='497 Main St', unit_number='103', city='Boston', county='Boston County', state='MA', zip_code=62167, extended_zip_code=Row(sector=35, segment=35))), Row(employee_id=1039, dept_id=50, first_name='Fiona', middle_name=None, last_name='Davis', phone_num=Row(country_code=1, area_code=533, exchange_code=263, subscriber_number=5708), email_id='fiona.davis@company.com', address=Row(street_name='126 Oak St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=30944, extended_zip_code=Row(sector=17, segment=76))), Row(employee_id=1040, dept_id=40, first_name='Fiona', middle_name='Yancy', last_name='Miller', phone_num=Row(country_code=1, area_code=396, exchange_code=621, subscriber_number=7595), email_id='fiona.miller@company.com', address=Row(street_name='248 Main St', unit_number=None, city='Seattle', county='Seattle County', state='WA', zip_code=87209, extended_zip_code=Row(sector=82, segment=38))), Row(employee_id=1041, dept_id=30, first_name='George', middle_name='Yancy', last_name='Smith', phone_num=Row(country_code=1, area_code=485, exchange_code=647, subscriber_number=1069), email_id='george.smith@company.com', address=Row(street_name='575 Oak St', unit_number=None, city='Seattle', county='Seattle County', state='WA', zip_code=10016, extended_zip_code=Row(sector=52, segment=14))), Row(employee_id=1042, dept_id=10, first_name='Alice', middle_name=None, last_name='Anderson', phone_num=Row(country_code=1, area_code=736, exchange_code=637, subscriber_number=7867), email_id='alice.anderson@company.com', address=Row(street_name='856 Oak St', unit_number='20', city='Seattle', county='Seattle County', state='WA', zip_code=50347, extended_zip_code=Row(sector=40, segment=22))), Row(employee_id=1043, dept_id=40, first_name='George', middle_name='Xavier', last_name='Smith', phone_num=Row(country_code=1, area_code=876, exchange_code=283, subscriber_number=1066), email_id='george.smith@company.com', address=Row(street_name='390 Pine St', unit_number='156', city='Austin', county='Austin County', state='TX', zip_code=54735, extended_zip_code=Row(sector=14, segment=22))), Row(employee_id=1044, dept_id=40, first_name='Bob', middle_name='Xavier', last_name='Moore', phone_num=Row(country_code=1, area_code=338, exchange_code=664, subscriber_number=1705), email_id='bob.moore@company.com', address=Row(street_name='733 Oak St', unit_number='158', city='New York', county='New York County', state='NY', zip_code=58129, extended_zip_code=Row(sector=89, segment=76))), Row(employee_id=1045, dept_id=20, first_name='Jack', middle_name=None, last_name='Moore', phone_num=Row(country_code=1, area_code=987, exchange_code=394, subscriber_number=4734), email_id='jack.moore@company.com', address=Row(street_name='928 Pine St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=99384, extended_zip_code=Row(sector=17, segment=85))), Row(employee_id=1046, dept_id=40, first_name='Charlie', middle_name=None, last_name='Smith', phone_num=Row(country_code=1, area_code=211, exchange_code=213, subscriber_number=1074), email_id='charlie.smith@company.com', address=Row(street_name='991 Main St', unit_number='92', city='Chicago', county='Chicago County', state='IL', zip_code=66778, extended_zip_code=Row(sector=53, segment=97))), Row(employee_id=1047, dept_id=40, first_name='Alice', middle_name='Yancy', last_name='Anderson', phone_num=Row(country_code=1, area_code=415, exchange_code=895, subscriber_number=1069), email_id='alice.anderson@company.com', address=Row(street_name='857 Oak St', unit_number='168', city='Denver', county='Denver County', state='CO', zip_code=52615, extended_zip_code=Row(sector=15, segment=10))), Row(employee_id=1048, dept_id=10, first_name='Jack', middle_name='Yancy', last_name='Taylor', phone_num=Row(country_code=1, area_code=632, exchange_code=756, subscriber_number=8269), email_id='jack.taylor@company.com', address=Row(street_name='445 Main St', unit_number=None, city='Seattle', county='Seattle County', state='WA', zip_code=97822, extended_zip_code=Row(sector=73, segment=53))), Row(employee_id=1049, dept_id=20, first_name='Ethan', middle_name=None, last_name='Williams', phone_num=Row(country_code=1, area_code=905, exchange_code=862, subscriber_number=9138), email_id='ethan.williams@company.com', address=Row(street_name='286 Oak St', unit_number='84', city='Seattle', county='Seattle County', state='WA', zip_code=31239, extended_zip_code=Row(sector=85, segment=69))), Row(employee_id=1050, dept_id=30, first_name='Jack', middle_name=None, last_name='Taylor', phone_num=Row(country_code=1, area_code=205, exchange_code=203, subscriber_number=6659), email_id='jack.taylor@company.com', address=Row(street_name='891 Pine St', unit_number='129', city='Denver', county='Denver County', state='CO', zip_code=47740, extended_zip_code=Row(sector=80, segment=39))), Row(employee_id=1051, dept_id=30, first_name='George', middle_name='Yancy', last_name='Smith', phone_num=Row(country_code=1, area_code=569, exchange_code=852, subscriber_number=5449), email_id='george.smith@company.com', address=Row(street_name='819 Main St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=77065, extended_zip_code=Row(sector=13, segment=72))), Row(employee_id=1052, dept_id=20, first_name='Charlie', middle_name='Xavier', last_name='Brown', phone_num=Row(country_code=1, area_code=592, exchange_code=365, subscriber_number=7773), email_id='charlie.brown@company.com', address=Row(street_name='300 Pine St', unit_number='179', city='Boston', county='Boston County', state='MA', zip_code=63779, extended_zip_code=Row(sector=26, segment=14))), Row(employee_id=1053, dept_id=40, first_name='George', middle_name='Xavier', last_name='Jones', phone_num=Row(country_code=1, area_code=886, exchange_code=313, subscriber_number=7875), email_id='george.jones@company.com', address=Row(street_name='717 Main St', unit_number='21', city='Austin', county='Austin County', state='TX', zip_code=11743, extended_zip_code=Row(sector=75, segment=13))), Row(employee_id=1054, dept_id=40, first_name='Alice', middle_name=None, last_name='Miller', phone_num=Row(country_code=1, area_code=282, exchange_code=975, subscriber_number=9886), email_id='alice.miller@company.com', address=Row(street_name='797 Main St', unit_number=None, city='Chicago', county='Chicago County', state='IL', zip_code=70508, extended_zip_code=Row(sector=89, segment=84))), Row(employee_id=1055, dept_id=40, first_name='Bob', middle_name=None, last_name='Williams', phone_num=Row(country_code=1, area_code=767, exchange_code=794, subscriber_number=5531), email_id='bob.williams@company.com', address=Row(street_name='447 Oak St', unit_number=None, city='Chicago', county='Chicago County', state='IL', zip_code=56480, extended_zip_code=Row(sector=10, segment=36))), Row(employee_id=1056, dept_id=20, first_name='Ethan', middle_name=None, last_name='Smith', phone_num=Row(country_code=1, area_code=929, exchange_code=401, subscriber_number=3016), email_id='ethan.smith@company.com', address=Row(street_name='173 Pine St', unit_number='4', city='New York', county='New York County', state='NY', zip_code=18647, extended_zip_code=Row(sector=90, segment=20))), Row(employee_id=1057, dept_id=50, first_name='Charlie', middle_name='Xavier', last_name='Brown', phone_num=Row(country_code=1, area_code=426, exchange_code=931, subscriber_number=6918), email_id='charlie.brown@company.com', address=Row(street_name='199 Main St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=19495, extended_zip_code=Row(sector=49, segment=29))), Row(employee_id=1058, dept_id=50, first_name='Diana', middle_name='Yancy', last_name='Smith', phone_num=Row(country_code=1, area_code=683, exchange_code=768, subscriber_number=4078), email_id='diana.smith@company.com', address=Row(street_name='822 Pine St', unit_number=None, city='New York', county='New York County', state='NY', zip_code=77093, extended_zip_code=Row(sector=47, segment=73))), Row(employee_id=1059, dept_id=40, first_name='Diana', middle_name='Yancy', last_name='Williams', phone_num=Row(country_code=1, area_code=873, exchange_code=821, subscriber_number=9083), email_id='diana.williams@company.com', address=Row(street_name='905 Oak St', unit_number='27', city='New York', county='New York County', state='NY', zip_code=81457, extended_zip_code=Row(sector=67, segment=58))), Row(employee_id=1060, dept_id=10, first_name='Hannah', middle_name='Yancy', last_name='Anderson', phone_num=Row(country_code=1, area_code=669, exchange_code=311, subscriber_number=1607), email_id='hannah.anderson@company.com', address=Row(street_name='528 Pine St', unit_number='108', city='New York', county='New York County', state='NY', zip_code=27639, extended_zip_code=Row(sector=63, segment=95))), Row(employee_id=1061, dept_id=10, first_name='Charlie', middle_name='Yancy', last_name='Williams', phone_num=Row(country_code=1, area_code=961, exchange_code=273, subscriber_number=1307), email_id='charlie.williams@company.com', address=Row(street_name='424 Pine St', unit_number='59', city='Seattle', county='Seattle County', state='WA', zip_code=89880, extended_zip_code=Row(sector=74, segment=84))), Row(employee_id=1062, dept_id=50, first_name='George', middle_name='Yancy', last_name='Miller', phone_num=Row(country_code=1, area_code=664, exchange_code=495, subscriber_number=9968), email_id='george.miller@company.com', address=Row(street_name='926 Pine St', unit_number=None, city='Seattle', county='Seattle County', state='WA', zip_code=26802, extended_zip_code=Row(sector=82, segment=57))), Row(employee_id=1063, dept_id=50, first_name='George', middle_name=None, last_name='Wilson', phone_num=Row(country_code=1, area_code=484, exchange_code=740, subscriber_number=3647), email_id='george.wilson@company.com', address=Row(street_name='661 Pine St', unit_number='4', city='Boston', county='Boston County', state='MA', zip_code=99017, extended_zip_code=Row(sector=52, segment=48))), Row(employee_id=1064, dept_id=30, first_name='Alice', middle_name=None, last_name='Smith', phone_num=Row(country_code=1, area_code=667, exchange_code=551, subscriber_number=7191), email_id='alice.smith@company.com', address=Row(street_name='229 Pine St', unit_number='178', city='Austin', county='Austin County', state='TX', zip_code=96479, extended_zip_code=Row(sector=65, segment=97))), Row(employee_id=1065, dept_id=40, first_name='Ivy', middle_name='Xavier', last_name='Brown', phone_num=Row(country_code=1, area_code=967, exchange_code=149, subscriber_number=6346), email_id='ivy.brown@company.com', address=Row(street_name='142 Oak St', unit_number=None, city='Boston', county='Boston County', state='MA', zip_code=52353, extended_zip_code=Row(sector=85, segment=48))), Row(employee_id=1066, dept_id=40, first_name='Jack', middle_name=None, last_name='Moore', phone_num=Row(country_code=1, area_code=725, exchange_code=460, subscriber_number=3054), email_id='jack.moore@company.com', address=Row(street_name='260 Oak St', unit_number=None, city='Austin', county='Austin County', state='TX', zip_code=47298, extended_zip_code=Row(sector=84, segment=91))), Row(employee_id=1067, dept_id=10, first_name='Ivy', middle_name=None, last_name='Miller', phone_num=Row(country_code=1, area_code=584, exchange_code=452, subscriber_number=7388), email_id='ivy.miller@company.com', address=Row(street_name='419 Pine St', unit_number='167', city='Chicago', county='Chicago County', state='IL', zip_code=46654, extended_zip_code=Row(sector=52, segment=85))), Row(employee_id=1068, dept_id=20, first_name='Charlie', middle_name='Yancy', last_name='Miller', phone_num=Row(country_code=1, area_code=337, exchange_code=558, subscriber_number=7181), email_id='charlie.miller@company.com', address=Row(street_name='761 Oak St', unit_number='18', city='Seattle', county='Seattle County', state='WA', zip_code=66694, extended_zip_code=Row(sector=46, segment=42))), Row(employee_id=1069, dept_id=40, first_name='Charlie', middle_name='Xavier', last_name='Moore', phone_num=Row(country_code=1, area_code=280, exchange_code=620, subscriber_number=9497), email_id='charlie.moore@company.com', address=Row(street_name='561 Pine St', unit_number=None, city='Austin', county='Austin County', state='TX', zip_code=99333, extended_zip_code=Row(sector=17, segment=79))), Row(employee_id=1070, dept_id=40, first_name='Fiona', middle_name='Xavier', last_name='Brown', phone_num=Row(country_code=1, area_code=476, exchange_code=573, subscriber_number=2387), email_id='fiona.brown@company.com', address=Row(street_name='837 Pine St', unit_number='171', city='Seattle', county='Seattle County', state='WA', zip_code=63120, extended_zip_code=Row(sector=33, segment=78))), Row(employee_id=1071, dept_id=10, first_name='Ivy', middle_name='Xavier', last_name='Brown', phone_num=Row(country_code=1, area_code=284, exchange_code=292, subscriber_number=1305), email_id='ivy.brown@company.com', address=Row(street_name='159 Oak St', unit_number=None, city='Austin', county='Austin County', state='TX', zip_code=21516, extended_zip_code=Row(sector=88, segment=43))), Row(employee_id=1072, dept_id=20, first_name='Alice', middle_name='Xavier', last_name='Jones', phone_num=Row(country_code=1, area_code=405, exchange_code=545, subscriber_number=9765), email_id='alice.jones@company.com', address=Row(street_name='228 Oak St', unit_number='63', city='Boston', county='Boston County', state='MA', zip_code=20502, extended_zip_code=Row(sector=35, segment=54))), Row(employee_id=1073, dept_id=50, first_name='Charlie', middle_name='Yancy', last_name='Jones', phone_num=Row(country_code=1, area_code=571, exchange_code=953, subscriber_number=1252), email_id='charlie.jones@company.com', address=Row(street_name='895 Pine St', unit_number='49', city='Boston', county='Boston County', state='MA', zip_code=68045, extended_zip_code=Row(sector=69, segment=78))), Row(employee_id=1074, dept_id=40, first_name='Ivy', middle_name='Xavier', last_name='Miller', phone_num=Row(country_code=1, area_code=280, exchange_code=179, subscriber_number=5572), email_id='ivy.miller@company.com', address=Row(street_name='746 Oak St', unit_number=None, city='Austin', county='Austin County', state='TX', zip_code=52231, extended_zip_code=Row(sector=75, segment=24))), Row(employee_id=1075, dept_id=40, first_name='Bob', middle_name='Yancy', last_name='Jones', phone_num=Row(country_code=1, area_code=739, exchange_code=529, subscriber_number=8945), email_id='bob.jones@company.com', address=Row(street_name='648 Main St', unit_number=None, city='Austin', county='Austin County', state='TX', zip_code=91451, extended_zip_code=Row(sector=77, segment=94))), Row(employee_id=1076, dept_id=10, first_name='Hannah', middle_name=None, last_name='Moore', phone_num=Row(country_code=1, area_code=750, exchange_code=822, subscriber_number=3219), email_id='hannah.moore@company.com', address=Row(street_name='513 Main St', unit_number=None, city='Chicago', county='Chicago County', state='IL', zip_code=60642, extended_zip_code=Row(sector=61, segment=12))), Row(employee_id=1077, dept_id=30, first_name='Bob', middle_name=None, last_name='Jones', phone_num=Row(country_code=1, area_code=409, exchange_code=521, subscriber_number=6807), email_id='bob.jones@company.com', address=Row(street_name='413 Oak St', unit_number='18', city='Boston', county='Boston County', state='MA', zip_code=65724, extended_zip_code=Row(sector=26, segment=23))), Row(employee_id=1078, dept_id=40, first_name='Diana', middle_name=None, last_name='Moore', phone_num=Row(country_code=1, area_code=694, exchange_code=962, subscriber_number=4946), email_id='diana.moore@company.com', address=Row(street_name='407 Oak St', unit_number='146', city='Seattle', county='Seattle County', state='WA', zip_code=80539, extended_zip_code=Row(sector=65, segment=24))), Row(employee_id=1079, dept_id=50, first_name='Ethan', middle_name=None, last_name='Jones', phone_num=Row(country_code=1, area_code=207, exchange_code=208, subscriber_number=4365), email_id='ethan.jones@company.com', address=Row(street_name='904 Pine St', unit_number=None, city='Seattle', county='Seattle County', state='WA', zip_code=60950, extended_zip_code=Row(sector=50, segment=78))), Row(employee_id=1080, dept_id=20, first_name='George', middle_name=None, last_name='Miller', phone_num=Row(country_code=1, area_code=219, exchange_code=607, subscriber_number=4384), email_id='george.miller@company.com', address=Row(street_name='978 Oak St', unit_number='121', city='Austin', county='Austin County', state='TX', zip_code=73163, extended_zip_code=Row(sector=26, segment=60))), Row(employee_id=1081, dept_id=40, first_name='Ivy', middle_name=None, last_name='Anderson', phone_num=Row(country_code=1, area_code=543, exchange_code=780, subscriber_number=8148), email_id='ivy.anderson@company.com', address=Row(street_name='385 Main St', unit_number='50', city='New York', county='New York County', state='NY', zip_code=34548, extended_zip_code=Row(sector=91, segment=36))), Row(employee_id=1082, dept_id=50, first_name='Ivy', middle_name='Yancy', last_name='Wilson', phone_num=Row(country_code=1, area_code=977, exchange_code=548, subscriber_number=4349), email_id='ivy.wilson@company.com', address=Row(street_name='674 Pine St', unit_number=None, city='Chicago', county='Chicago County', state='IL', zip_code=59491, extended_zip_code=Row(sector=28, segment=54))), Row(employee_id=1083, dept_id=40, first_name='Alice', middle_name='Xavier', last_name='Taylor', phone_num=Row(country_code=1, area_code=662, exchange_code=539, subscriber_number=2272), email_id='alice.taylor@company.com', address=Row(street_name='966 Main St', unit_number='75', city='Chicago', county='Chicago County', state='IL', zip_code=70539, extended_zip_code=Row(sector=31, segment=17))), Row(employee_id=1084, dept_id=50, first_name='Ethan', middle_name=None, last_name='Anderson', phone_num=Row(country_code=1, area_code=983, exchange_code=421, subscriber_number=5224), email_id='ethan.anderson@company.com', address=Row(street_name='304 Oak St', unit_number=None, city='Boston', county='Boston County', state='MA', zip_code=86042, extended_zip_code=Row(sector=63, segment=20))), Row(employee_id=1085, dept_id=20, first_name='Jack', middle_name='Xavier', last_name='Wilson', phone_num=Row(country_code=1, area_code=765, exchange_code=207, subscriber_number=4473), email_id='jack.wilson@company.com', address=Row(street_name='219 Main St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=41360, extended_zip_code=Row(sector=68, segment=73))), Row(employee_id=1086, dept_id=40, first_name='Hannah', middle_name='Xavier', last_name='Williams', phone_num=Row(country_code=1, area_code=410, exchange_code=405, subscriber_number=4581), email_id='hannah.williams@company.com', address=Row(street_name='656 Pine St', unit_number='172', city='Boston', county='Boston County', state='MA', zip_code=45252, extended_zip_code=Row(sector=32, segment=81))), Row(employee_id=1087, dept_id=30, first_name='Jack', middle_name='Xavier', last_name='Moore', phone_num=Row(country_code=1, area_code=830, exchange_code=431, subscriber_number=3095), email_id='jack.moore@company.com', address=Row(street_name='656 Pine St', unit_number=None, city='Chicago', county='Chicago County', state='IL', zip_code=89774, extended_zip_code=Row(sector=92, segment=99))), Row(employee_id=1088, dept_id=50, first_name='Alice', middle_name='Yancy', last_name='Wilson', phone_num=Row(country_code=1, area_code=927, exchange_code=869, subscriber_number=8704), email_id='alice.wilson@company.com', address=Row(street_name='228 Main St', unit_number='17', city='Austin', county='Austin County', state='TX', zip_code=33803, extended_zip_code=Row(sector=38, segment=27))), Row(employee_id=1089, dept_id=10, first_name='Charlie', middle_name=None, last_name='Miller', phone_num=Row(country_code=1, area_code=771, exchange_code=149, subscriber_number=6263), email_id='charlie.miller@company.com', address=Row(street_name='975 Main St', unit_number=None, city='Boston', county='Boston County', state='MA', zip_code=16949, extended_zip_code=Row(sector=83, segment=21))), Row(employee_id=1090, dept_id=10, first_name='George', middle_name='Yancy', last_name='Williams', phone_num=Row(country_code=1, area_code=622, exchange_code=965, subscriber_number=5324), email_id='george.williams@company.com', address=Row(street_name='151 Main St', unit_number='133', city='New York', county='New York County', state='NY', zip_code=50401, extended_zip_code=Row(sector=71, segment=74))), Row(employee_id=1091, dept_id=10, first_name='Ivy', middle_name=None, last_name='Wilson', phone_num=Row(country_code=1, area_code=417, exchange_code=694, subscriber_number=9880), email_id='ivy.wilson@company.com', address=Row(street_name='273 Main St', unit_number=None, city='Boston', county='Boston County', state='MA', zip_code=84937, extended_zip_code=Row(sector=27, segment=88))), Row(employee_id=1092, dept_id=30, first_name='Charlie', middle_name='Xavier', last_name='Davis', phone_num=Row(country_code=1, area_code=343, exchange_code=156, subscriber_number=9873), email_id='charlie.davis@company.com', address=Row(street_name='585 Pine St', unit_number=None, city='Denver', county='Denver County', state='CO', zip_code=31370, extended_zip_code=Row(sector=38, segment=47))), Row(employee_id=1093, dept_id=50, first_name='Fiona', middle_name=None, last_name='Wilson', phone_num=Row(country_code=1, area_code=732, exchange_code=188, subscriber_number=4322), email_id='fiona.wilson@company.com', address=Row(street_name='793 Oak St', unit_number=None, city='Seattle', county='Seattle County', state='WA', zip_code=83639, extended_zip_code=Row(sector=11, segment=12))), Row(employee_id=1094, dept_id=10, first_name='Diana', middle_name='Yancy', last_name='Moore', phone_num=Row(country_code=1, area_code=851, exchange_code=536, subscriber_number=3781), email_id='diana.moore@company.com', address=Row(street_name='758 Main St', unit_number='77', city='Boston', county='Boston County', state='MA', zip_code=44065, extended_zip_code=Row(sector=54, segment=17))), Row(employee_id=1095, dept_id=30, first_name='Diana', middle_name='Yancy', last_name='Moore', phone_num=Row(country_code=1, area_code=804, exchange_code=677, subscriber_number=6119), email_id='diana.moore@company.com', address=Row(street_name='868 Oak St', unit_number=None, city='Seattle', county='Seattle County', state='WA', zip_code=15817, extended_zip_code=Row(sector=92, segment=19))), Row(employee_id=1096, dept_id=50, first_name='Ethan', middle_name='Yancy', last_name='Brown', phone_num=Row(country_code=1, area_code=283, exchange_code=323, subscriber_number=4896), email_id='ethan.brown@company.com', address=Row(street_name='328 Pine St', unit_number=None, city='Boston', county='Boston County', state='MA', zip_code=37847, extended_zip_code=Row(sector=13, segment=22))), Row(employee_id=1097, dept_id=50, first_name='Jack', middle_name=None, last_name='Wilson', phone_num=Row(country_code=1, area_code=931, exchange_code=213, subscriber_number=4682), email_id='jack.wilson@company.com', address=Row(street_name='999 Pine St', unit_number='187', city='Boston', county='Boston County', state='MA', zip_code=79790, extended_zip_code=Row(sector=11, segment=29))), Row(employee_id=1098, dept_id=10, first_name='Diana', middle_name='Xavier', last_name='Moore', phone_num=Row(country_code=1, area_code=324, exchange_code=937, subscriber_number=2427), email_id='diana.moore@company.com', address=Row(street_name='550 Main St', unit_number='172', city='Chicago', county='Chicago County', state='IL', zip_code=76437, extended_zip_code=Row(sector=99, segment=36))), Row(employee_id=1099, dept_id=40, first_name='Diana', middle_name='Yancy', last_name='Smith', phone_num=Row(country_code=1, area_code=983, exchange_code=416, subscriber_number=6180), email_id='diana.smith@company.com', address=Row(street_name='385 Oak St', unit_number=None, city='New York', county='New York County', state='NY', zip_code=64103, extended_zip_code=Row(sector=74, segment=37))), Row(employee_id=1100, dept_id=20, first_name='George', middle_name=None, last_name='Taylor', phone_num=Row(country_code=1, area_code=998, exchange_code=127, subscriber_number=3613), email_id='george.taylor@company.com', address=Row(street_name='674 Pine St', unit_number='25', city='Austin', county='Austin County', state='TX', zip_code=70325, extended_zip_code=Row(sector=49, segment=51)))]\n"
     ]
    }
   ],
   "source": [
    "# Create a test data\n",
    "employee_data = []\n",
    "for i in range(1, 101): # 100 employees\n",
    "    emp_id = 1000 + i\n",
    "    dept_id = random.choice(DEPT_IDS) # Ensure dept_id is valid\n",
    "    \n",
    "    first = random.choice(FIRST_NAMES)\n",
    "    last = random.choice(LAST_NAMES)\n",
    "    \n",
    "    city = random.choice(CITIES)\n",
    "    state = STATES[CITIES.index(city)]\n",
    "    \n",
    "    employee_data.append(Row(\n",
    "        employee_id=emp_id,\n",
    "        dept_id=dept_id,\n",
    "        first_name=first,\n",
    "        middle_name=random.choice([None, \"Xavier\", \"Yancy\"]), # Allow for null middle_name\n",
    "        last_name=last,\n",
    "        \n",
    "        # Nested Phone Structure (Non-Nullable fields generated)\n",
    "        phone_num=Row(\n",
    "            country_code=1,\n",
    "            area_code=random.randint(200, 999),\n",
    "            exchange_code=random.randint(100, 999),\n",
    "            subscriber_number=random.randint(1000, 9999)\n",
    "        ),\n",
    "        \n",
    "        email_id=f\"{first.lower()}.{last.lower()}@company.com\",\n",
    "        \n",
    "        # Nested Address Structure (Non-Nullable fields generated)\n",
    "        address=generate_address(city, state)\n",
    "    ))\n",
    "\n",
    "print(employee_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8259342d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+-----------+---------+-------------------+--------------------+--------------------+\n",
      "|employee_id|dept_id|first_name|middle_name|last_name|          phone_num|            email_id|             address|\n",
      "+-----------+-------+----------+-----------+---------+-------------------+--------------------+--------------------+\n",
      "|       1001|     30|    Hannah|       NULL|   Taylor|{1, 600, 372, 2883}|hannah.taylor@com...|{530 Pine St, NUL...|\n",
      "|       1002|     10|     Alice|     Xavier|    Moore|{1, 979, 942, 8967}|alice.moore@compa...|{126 Oak St, NULL...|\n",
      "|       1003|     50|   Charlie|       NULL|    Brown|{1, 640, 291, 4687}|charlie.brown@com...|{299 Oak St, NULL...|\n",
      "|       1004|     30|     Ethan|     Xavier|    Davis|{1, 899, 213, 4400}|ethan.davis@compa...|{744 Oak St, 105,...|\n",
      "|       1005|     20|       Bob|       NULL|    Moore|{1, 817, 117, 4435}|bob.moore@company...|{760 Pine St, 116...|\n",
      "|       1006|     40|       Bob|      Yancy|    Moore|{1, 474, 519, 6813}|bob.moore@company...|{999 Oak St, NULL...|\n",
      "|       1007|     10|    Hannah|       NULL|   Taylor|{1, 910, 684, 8785}|hannah.taylor@com...|{718 Pine St, 179...|\n",
      "|       1008|     30|     Alice|      Yancy|    Davis|{1, 312, 782, 7781}|alice.davis@compa...|{155 Oak St, NULL...|\n",
      "|       1009|     30|      Jack|     Xavier| Williams|{1, 630, 321, 5364}|jack.williams@com...|{207 Main St, 46,...|\n",
      "|       1010|     40|      Jack|     Xavier|   Taylor|{1, 971, 878, 3574}|jack.taylor@compa...|{986 Main St, 93,...|\n",
      "|       1011|     50|     Alice|       NULL|    Moore|{1, 296, 278, 7752}|alice.moore@compa...|{864 Oak St, 38, ...|\n",
      "|       1012|     50|     Alice|       NULL|   Miller|{1, 513, 441, 7931}|alice.miller@comp...|{943 Oak St, NULL...|\n",
      "|       1013|     20|       Bob|      Yancy|    Davis|{1, 940, 849, 4932}|bob.davis@company...|{624 Oak St, NULL...|\n",
      "|       1014|     50|     Ethan|       NULL|   Wilson|{1, 448, 972, 4364}|ethan.wilson@comp...|{908 Oak St, 54, ...|\n",
      "|       1015|     20|     Alice|       NULL| Anderson|{1, 870, 500, 2319}|alice.anderson@co...|{617 Pine St, NUL...|\n",
      "|       1016|     50|     Alice|       NULL| Anderson|{1, 500, 524, 1910}|alice.anderson@co...|{378 Oak St, 33, ...|\n",
      "|       1017|     10|     Fiona|       NULL|   Miller|{1, 804, 111, 9261}|fiona.miller@comp...|{153 Oak St, 46, ...|\n",
      "|       1018|     40|    George|      Yancy|    Smith|{1, 474, 840, 7410}|george.smith@comp...|{897 Oak St, 119,...|\n",
      "|       1019|     30|       Bob|     Xavier|    Smith|{1, 808, 338, 9707}|bob.smith@company...|{708 Pine St, NUL...|\n",
      "|       1020|     50|     Alice|     Xavier| Anderson|{1, 554, 681, 4325}|alice.anderson@co...|{731 Oak St, NULL...|\n",
      "+-----------+-------+----------+-----------+---------+-------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_employee = spark.createDataFrame(employee_data, schema=employee_schema)\n",
    "\n",
    "df_employee.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "53f5fee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+----------------+--------------------+\n",
      "|dept_id|  dept_name|dept_head_emp_id|         loc_address|\n",
      "+-------+-----------+----------------+--------------------+\n",
      "|     10|Engineering|            1010|{833 Oak St, NULL...|\n",
      "|     20|      Sales|            1020|{857 Main St, 94,...|\n",
      "|     30|         HR|            1030|{467 Main St, NUL...|\n",
      "|     40|    Finance|            1040|{627 Pine St, 96,...|\n",
      "|     50|  Marketing|            1050|{495 Oak St, 110,...|\n",
      "+-------+-----------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_department = spark.createDataFrame(dept_data, schema=dept_schema)\n",
    "\n",
    "df_department.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "20355873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+----------+-----------+---------+-------------------+--------------------+--------------------+\n",
      "|employee_id|dept_id|first_name|middle_name|last_name|          phone_num|            email_id|             address|\n",
      "+-----------+-------+----------+-----------+---------+-------------------+--------------------+--------------------+\n",
      "|       1010|     40|      Jack|     Xavier|   Taylor|{1, 971, 878, 3574}|jack.taylor@compa...|{986 Main St, 93,...|\n",
      "+-----------+-------+----------+-----------+---------+-------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_employee.filter(col(\"employee_id\") == 1010).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "3900d4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|       1010|\n",
      "|       1020|\n",
      "|       1030|\n",
      "|       1040|\n",
      "|       1050|\n",
      "+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/03 13:58:13 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 920639 ms exceeds timeout 120000 ms\n",
      "26/01/03 13:58:13 WARN SparkContext: Killing executors is not supported by current scheduler.\n",
      "26/01/03 13:58:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 13:58:14 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:545)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:369)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:310)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 14:31:24 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 14:31:24 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:545)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:369)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:310)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 14:46:34 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:545)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:369)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:310)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 14:46:34 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 14:46:44 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 14:46:44 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:545)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:369)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:310)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 15:01:56 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 15:01:56 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:545)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:369)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:310)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 15:02:17 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 15:02:17 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:545)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:369)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:310)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 15:06:27 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:545)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:369)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:310)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 15:06:27 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 15:06:37 WARN Executor: Issue communicating with driver in heartbeater\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:85)\n",
      "\tat org.apache.spark.storage.BlockManagerMaster.registerBlockManager(BlockManagerMaster.scala:81)\n",
      "\tat org.apache.spark.storage.BlockManager.reregister(BlockManager.scala:674)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1324)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:322)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1941)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:545)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:369)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:310)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\t... 3 more\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n",
      "26/01/03 15:06:37 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exception thrown in awaitResult: \n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:53)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:359)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRefByURI(RpcEnv.scala:102)\n",
      "\tat org.apache.spark.rpc.RpcEnv.setupEndpointRef(RpcEnv.scala:110)\n",
      "\tat org.apache.spark.util.RpcUtils$.makeDriverRef(RpcUtils.scala:36)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.driverEndpoint$lzycompute(BlockManagerMasterEndpoint.scala:132)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$driverEndpoint(BlockManagerMasterEndpoint.scala:131)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$lzycompute$1(BlockManagerMasterEndpoint.scala:707)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.isExecutorAlive$1(BlockManagerMasterEndpoint.scala:706)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint.org$apache$spark$storage$BlockManagerMasterEndpoint$$register(BlockManagerMasterEndpoint.scala:746)\n",
      "\tat org.apache.spark.storage.BlockManagerMasterEndpoint$$anonfun$receiveAndReply$1.applyOrElse(BlockManagerMasterEndpoint.scala:141)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:216)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:76)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:42)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1090)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:614)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1474)\n",
      "Caused by: org.apache.spark.rpc.RpcEndpointNotFoundException: Cannot find endpoint: spark://CoarseGrainedScheduler@10.0.0.159:56362\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1(NettyRpcEnv.scala:151)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$asyncSetupEndpointRefByURI$1$adapted(NettyRpcEnv.scala:147)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:503)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:537)\n",
      "\tat scala.concurrent.ExecutionContext$parasitic$.execute(ExecutionContext.scala:222)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:368)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.trySuccess(Promise.scala:99)\n",
      "\tat scala.concurrent.Promise.trySuccess$(Promise.scala:99)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.trySuccess(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.onSuccess$1(NettyRpcEnv.scala:228)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5(NettyRpcEnv.scala:242)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcEnv.$anonfun$askAbortable$5$adapted(NettyRpcEnv.scala:241)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.run(Promise.scala:517)\n",
      "\tat org.apache.spark.util.ThreadUtils$$anon$1.execute(ThreadUtils.scala:99)\n",
      "\tat scala.concurrent.impl.ExecutionContextImpl.execute(ExecutionContextImpl.scala:21)\n",
      "\tat scala.concurrent.impl.Promise$Transformation.submitWithValue(Promise.scala:462)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.submitWithValue(Promise.scala:371)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete0(Promise.scala:295)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:288)\n",
      "\tat scala.concurrent.Promise.complete(Promise.scala:57)\n",
      "\tat scala.concurrent.Promise.complete$(Promise.scala:56)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:104)\n",
      "\tat scala.concurrent.Promise.success(Promise.scala:91)\n",
      "\tat scala.concurrent.Promise.success$(Promise.scala:91)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.success(Promise.scala:104)\n",
      "\tat org.apache.spark.rpc.netty.LocalNettyRpcCallContext.send(NettyRpcCallContext.scala:50)\n",
      "\tat org.apache.spark.rpc.netty.NettyRpcCallContext.reply(NettyRpcCallContext.scala:32)\n",
      "\tat org.apache.spark.rpc.netty.RpcEndpointVerifier$$anonfun$receiveAndReply$1.applyOrElse(RpcEndpointVerifier.scala:31)\n",
      "\t... 8 more\n"
     ]
    }
   ],
   "source": [
    "df_employee.alias(\"e\").join(df_department.alias(\"d\"), on=(col(\"e.employee_id\") == col(\"d.dept_head_emp_id\")), how=\"inner\").select(col(\"e.employee_id\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b33f2413",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "355fc7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d95fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/19 14:03:21 WARN Utils: Your hostname, Pulastyas-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 10.0.0.40 instead (on interface en0)\n",
      "26/01/19 14:03:21 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/19 14:03:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('Car Launch').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b4ccc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.options(header=True, inferSchema=True).csv('data/input/car_launch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c1c9c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+\n",
      "|year|company_name|product_name|\n",
      "+----+------------+------------+\n",
      "|2019|      Toyota|      Avalon|\n",
      "|2019|      Toyota|       Camry|\n",
      "|2020|      Toyota|     Corolla|\n",
      "|2019|       Honda|      Accord|\n",
      "|2019|       Honda|    Passport|\n",
      "|2019|       Honda|        CR-V|\n",
      "|2020|       Honda|       Pilot|\n",
      "|2019|       Honda|       Civic|\n",
      "|2020|   Chevrolet| Trailblazer|\n",
      "|2020|   Chevrolet|        Trax|\n",
      "|2019|   Chevrolet|    Traverse|\n",
      "|2020|   Chevrolet|      Blazer|\n",
      "|2019|        Ford|        Figo|\n",
      "|2020|        Ford|      Aspire|\n",
      "|2019|        Ford|   Endeavour|\n",
      "|2020|        Jeep|    Wrangler|\n",
      "|2020|        Jeep|    Cherokee|\n",
      "|2020|        Jeep|     Compass|\n",
      "|2019|        Jeep|    Renegade|\n",
      "|2019|        Jeep|   Gladiator|\n",
      "+----+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb4be7e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|company_name|net_products|\n",
      "+------------+------------+\n",
      "|        Jeep|           1|\n",
      "|   Chevrolet|           2|\n",
      "|       Honda|          -3|\n",
      "|      Toyota|          -1|\n",
      "|        Ford|          -1|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = df \\\n",
    "    .groupBy('company_name') \\\n",
    "    .pivot('year') \\\n",
    "    .agg(F.count('product_name')) \\\n",
    "    .withColumn('net_products', F.col('2020') - F.col('2019')) \\\n",
    "    .select('company_name', 'net_products')\n",
    "    \n",
    "result.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "77b15c8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+-------+\n",
      "|record_date|account_id|user_id|\n",
      "+-----------+----------+-------+\n",
      "| 2021-01-01|        A1|     U1|\n",
      "| 2021-01-01|        A1|     U2|\n",
      "| 2021-01-06|        A1|     U3|\n",
      "| 2021-01-02|        A1|     U1|\n",
      "| 2020-12-24|        A1|     U2|\n",
      "| 2020-01-03|        A1|     U1|\n",
      "| 2020-12-09|        A1|     U1|\n",
      "| 2021-01-10|        A2|     U4|\n",
      "| 2021-01-11|        A2|     U4|\n",
      "| 2021-01-12|        A2|     U4|\n",
      "| 2021-01-15|        A2|     U5|\n",
      "| 2020-12-17|        A2|     U4|\n",
      "| 2020-12-25|        A3|     U6|\n",
      "| 2020-12-25|        A3|     U6|\n",
      "| 2020-12-25|        A3|     U6|\n",
      "| 2020-12-06|        A3|     U7|\n",
      "| 2020-12-06|        A3|     U6|\n",
      "| 2021-01-14|        A3|     U6|\n",
      "| 2021-02-07|        A1|     U1|\n",
      "| 2021-02-10|        A1|     U2|\n",
      "+-----------+----------+-------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "login = spark.read.options(header=True, inferSchema=True).csv('data/input/user_login.csv')\n",
    "\n",
    "login.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6cf5c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|user_id|\n",
      "+-------+\n",
      "|     U4|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result1 = login.dropDuplicates()\\\n",
    "    .withColumn('record_date', F.to_date('record_date', 'YYYY-MM-DD'))\\\n",
    "    .withColumn('rank', F.row_number().over(Window.partitionBy('user_id').orderBy('record_date')))\\\n",
    "    .withColumn('consecutive_days', F.date_sub(F.col('record_date'), F.col('rank') - 1))\\\n",
    "    .groupBy('user_id', 'consecutive_days')\\\n",
    "    .agg(F.count('*').alias('counter'))\\\n",
    "    .filter(F.col('counter') >= 3)\\\n",
    "    .select('user_id')\\\n",
    "    .drop_duplicates()\n",
    "\n",
    "result1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3fc9da5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mspark\u001b[49m.stop()\n",
      "\u001b[31mNameError\u001b[39m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

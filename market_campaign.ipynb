{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4e3b8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f618418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "26/01/21 20:24:06 WARN Utils: Your hostname, Pulastyas-MacBook-Air.local, resolves to a loopback address: 127.0.0.1; using 10.0.0.40 instead (on interface en0)\n",
      "26/01/21 20:24:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "26/01/21 20:24:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('market campaign').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49cd561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0dc6ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+-----+\n",
      "|user_id|created_at|product_id|quantity|price|\n",
      "+-------+----------+----------+--------+-----+\n",
      "|10     |2019-01-01|101       |3       |55   |\n",
      "|10     |2019-01-02|119       |5       |29   |\n",
      "|10     |2019-03-31|111       |2       |149  |\n",
      "|11     |2019-01-02|105       |3       |234  |\n",
      "|11     |2019-03-31|120       |3       |99   |\n",
      "|12     |2019-01-02|112       |2       |200  |\n",
      "|12     |2019-03-31|110       |2       |299  |\n",
      "|13     |2019-01-05|113       |1       |67   |\n",
      "|13     |2019-03-31|118       |3       |35   |\n",
      "|14     |2019-01-06|109       |5       |199  |\n",
      "|14     |2019-01-06|107       |2       |27   |\n",
      "|14     |2019-03-31|112       |3       |200  |\n",
      "|15     |2019-01-08|105       |4       |234  |\n",
      "|15     |2019-01-09|110       |4       |299  |\n",
      "|15     |2019-03-31|116       |2       |499  |\n",
      "|16     |2019-01-10|113       |2       |67   |\n",
      "|16     |2019-03-31|107       |4       |27   |\n",
      "|17     |2019-01-11|116       |2       |499  |\n",
      "|17     |2019-03-31|104       |1       |154  |\n",
      "|18     |2019-01-12|114       |2       |248  |\n",
      "+-------+----------+----------+--------+-----+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "mc = spark.read.options(header=True, inferSchema=True)\\\n",
    "    .csv('data/input/marketing_campaign.csv').dropDuplicates(['user_id', 'created_at', 'product_id'])\\\n",
    "    .orderBy(F.col('user_id'), F.col('created_at'))\\\n",
    "    .coalesce(1)\n",
    "    \n",
    "mc.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0231b500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "Repartition 1, false\n",
      "+- Sort [user_id#177 ASC NULLS FIRST, created_at#178 ASC NULLS FIRST], true\n",
      "   +- Deduplicate [user_id#177, created_at#178, product_id#179]\n",
      "      +- Relation [user_id#177,created_at#178,product_id#179,quantity#180,price#181] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "user_id: int, created_at: date, product_id: int, quantity: int, price: int\n",
      "Repartition 1, false\n",
      "+- Sort [user_id#177 ASC NULLS FIRST, created_at#178 ASC NULLS FIRST], true\n",
      "   +- Deduplicate [user_id#177, created_at#178, product_id#179]\n",
      "      +- Relation [user_id#177,created_at#178,product_id#179,quantity#180,price#181] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Repartition 1, false\n",
      "+- Sort [user_id#177 ASC NULLS FIRST, created_at#178 ASC NULLS FIRST], true\n",
      "   +- Aggregate [user_id#177, created_at#178, product_id#179], [user_id#177, created_at#178, product_id#179, first(quantity#180, false) AS quantity#233, first(price#181, false) AS price#235]\n",
      "      +- Relation [user_id#177,created_at#178,product_id#179,quantity#180,price#181] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Coalesce 1\n",
      "   +- Sort [user_id#177 ASC NULLS FIRST, created_at#178 ASC NULLS FIRST], true, 0\n",
      "      +- Exchange rangepartitioning(user_id#177 ASC NULLS FIRST, created_at#178 ASC NULLS FIRST, 1), ENSURE_REQUIREMENTS, [plan_id=413]\n",
      "         +- HashAggregate(keys=[user_id#177, created_at#178, product_id#179], functions=[first(quantity#180, false), first(price#181, false)], output=[user_id#177, created_at#178, product_id#179, quantity#233, price#235])\n",
      "            +- Exchange hashpartitioning(user_id#177, created_at#178, product_id#179, 1), ENSURE_REQUIREMENTS, [plan_id=410]\n",
      "               +- HashAggregate(keys=[user_id#177, created_at#178, product_id#179], functions=[partial_first(quantity#180, false), partial_first(price#181, false)], output=[user_id#177, created_at#178, product_id#179, first#240, valueSet#241, first#242, valueSet#243])\n",
      "                  +- FileScan csv [user_id#177,created_at#178,product_id#179,quantity#180,price#181] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/Users/pulastyanarainpandey/Documents/pyspark-practice/data/input..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<user_id:int,created_at:date,product_id:int,quantity:int,price:int>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mc.explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d8a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
